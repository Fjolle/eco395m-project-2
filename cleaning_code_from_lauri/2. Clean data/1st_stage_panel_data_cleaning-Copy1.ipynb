{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1st Stage Panel Data Cleaning\n",
    "\n",
    "This notebook performs the first round of data cleaning tasks on the final long form data produced from the Aggregate Listings Data notebook. These tasks include:\n",
    "\n",
    "* 1. **Preliminary cleaning:** Includes de-stringing prices, formatting dates and computing variable lags and leads\n",
    "* 2. **Exploring missing values:** Creates functions for understanding NaN values and fills in time-invariant characteristics of a listing\n",
    "* 3. **Neighborhood categorization:** Assign neighborhoods to listings based on their GPS coordinates \n",
    "* 4. **'Calendar Update' formatting:** Creating numerical measure for how often an Airbnb listing is updated by the property host\n",
    "* 5. **Additional variable creation:** Additional variables are created for further anaylsis\n",
    "* 6. **Creating drop criteria for observations:** Primary focus of drop criteria is to drop dormant listings, price outliers and full-time hotels\n",
    "\n",
    "Once these tasks are complete, the Notebook saves the cleaned data into a compressed csv.gz file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import operator # This allows one to pass operators into a Python function\n",
    "import mpu # For distance calculation\n",
    "from scipy import stats # Used to find modal value of geographic values\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select city to work with\n",
    "\n",
    "city_folder = '/united-states_new-york-city'\n",
    "city_abbrev = 'NYC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Universal directory setup\n",
    "cwd1 = os.getcwd() \n",
    "\n",
    "# Go up one directory level\n",
    "os.chdir('..')\n",
    "cwd2 = os.getcwd()\n",
    "\n",
    "csv_raw_path = cwd2 + '/1. Download and compile data/'\n",
    "csv_save_path = cwd2 + '/Saved data/'\n",
    "\n",
    "# Revert to preliminary directory\n",
    "os.chdir(cwd1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     id  month  List_month last_scraped  host_id    host_name\n",
      "0  2595      0           1   2021-09-02   2845.0     Jennifer\n",
      "1  3831      0           1   2021-09-02   4869.0  LisaRoxanne\n",
      "2  5121      0           1   2021-09-02   7356.0        Garon\n",
      "3  5136      0           1   2021-09-02   7378.0      Rebecca\n",
      "4  5178      0           1   2021-09-02   8967.0     Shunichi\n",
      "5  5203      0           1   2021-09-02   7490.0    MaryEllen\n"
     ]
    }
   ],
   "source": [
    "# Read concatenated data\n",
    "os.chdir(csv_raw_path)\n",
    "\n",
    "listings_df = pd.read_csv(city_abbrev + '_Data_longALL_2021.csv.gz', low_memory=False)\n",
    "\n",
    "# Switch to other folder for saving data\n",
    "os.chdir(csv_save_path)\n",
    "\n",
    "# Show a snapshot of the dataframe\n",
    "print(listings_df.iloc[:6,:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'month', 'List_month', 'last_scraped', 'host_id', 'host_name',\n",
       "       'host_since', 'host_location', 'host_response_time',\n",
       "       'host_response_rate', 'host_is_superhost', 'host_listings_count',\n",
       "       'host_total_listings_count', 'neighbourhood', 'neighbourhood_cleansed',\n",
       "       'neighbourhood_group_cleansed', 'street', 'zipcode', 'latitude',\n",
       "       'longitude', 'is_location_exact', 'property_type', 'room_type',\n",
       "       'accommodates', 'bathrooms', 'bedrooms', 'beds', 'bed_type',\n",
       "       'square_feet', 'price', 'weekly_price', 'monthly_price',\n",
       "       'security_deposit', 'cleaning_fee', 'guests_included', 'extra_people',\n",
       "       'minimum_nights', 'maximum_nights', 'calendar_updated',\n",
       "       'calendar_last_scraped', 'has_availability', 'availability_30',\n",
       "       'availability_60', 'availability_90', 'availability_365',\n",
       "       'number_of_reviews', 'first_review', 'last_review',\n",
       "       'review_scores_rating', 'review_scores_accuracy',\n",
       "       'review_scores_cleanliness', 'review_scores_checkin',\n",
       "       'review_scores_communication', 'review_scores_location',\n",
       "       'review_scores_value', 'requires_license', 'license',\n",
       "       'instant_bookable', 'cancellation_policy',\n",
       "       'calculated_host_listings_count', 'reviews_per_month', 'amenities'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listings_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "# 1. Preliminary Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Destringing prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def destring_price(var):\n",
    "    \"\"\"\n",
    "    Destrings a passed variable.\n",
    "    \"\"\"\n",
    "    listings_df.loc[:, var] = listings_df[var].replace('[\\$,]', '', regex=True).astype(float)\n",
    "\n",
    "# This loop destrings price variables\n",
    "for var in ['price']:\n",
    "    \n",
    "    destring_price(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dates(var):\n",
    "    \"\"\"\n",
    "    This function converts date variables into datetime format.\n",
    "    \"\"\"\n",
    "    listings_df.loc[:, var] = pd.to_datetime(listings_df[var])\n",
    "    \n",
    "# ===============================================================    \n",
    "\n",
    "def dates_diff(var_name, var1, var2):\n",
    "    \"\"\"\n",
    "    Computes the difference between two date variables and assigns\n",
    "    the difference to a new variable of given name.\n",
    "    \"\"\"\n",
    "    listings_df.loc[:, var_name] = listings_df[var1] - listings_df[var2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a loop of date formatting\n",
    "for date_vars in ['last_scraped', 'host_since', 'first_review', 'last_review']:\n",
    "    format_dates(date_vars)\n",
    "    \n",
    "# Set the 'scrape_batch' as a modal date for a file being scraped in a CSV\n",
    "\n",
    "for m in listings_df['month'].unique():\n",
    "    listings_df.loc[listings_df['month'] == m, 'scrape_batch'] = listings_df[listings_df['month'] == m]['last_scraped'].mode().values[0]\n",
    "    listings_df.loc[:, 'scrape_batch'] = pd.to_datetime(listings_df['scrape_batch'])\n",
    "    \n",
    "# Create a Year-Month value for the scrape batch, this is largely used for graphing where one needs to aggregate by year-month\n",
    "listings_df.loc[:,\"batch_YRMO\"] = pd.to_datetime(listings_df['scrape_batch']).dt.to_period('M')    \n",
    "\n",
    "# Calculate different date differences\n",
    "dates_diff('days_since_rev', 'last_scraped', 'last_review')\n",
    "dates_diff('days_since_first_rev', 'last_scraped', 'first_review')\n",
    "dates_diff('host_length', 'last_scraped', 'host_since')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timedelta_formatter(var):\n",
    "    \"\"\"\n",
    "    This function formats the time delta for a passed variable.\n",
    "    \"\"\"\n",
    "    listings_df.loc[:, var] = pd.to_timedelta(listings_df[var]).dt.days\n",
    "    \n",
    "for deltas in ['days_since_rev', 'days_since_first_rev', 'host_length']:\n",
    "    timedelta_formatter(deltas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leads and lag creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lagsleads(var, lag_range, df, title):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function creates lag variables within a given range \n",
    "    for a given variable within a given dataframe. The title of these lag \n",
    "    variables is specified by title.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.sort_values(by = ['id', 'month']).copy()\n",
    "    \n",
    "    for i in range(-lag_range, lag_range + 1):\n",
    "        \n",
    "        if i == 0:\n",
    "            continue\n",
    "        \n",
    "        if i < 0:\n",
    "            df.loc[:, title + \"lead\" + str(abs(i)) ] = df.groupby('id')[var].shift(i).copy()\n",
    "                \n",
    "        if i > 0: \n",
    "            df.loc[:, title + \"lag\" + str(abs(i)) ] = df.groupby('id')[var].shift(i).copy()\n",
    "            \n",
    "    return df\n",
    "\n",
    "listings_df = create_lagsleads('List_month', 12, listings_df, \"List\")\n",
    "listings_df = create_lagsleads('availability_60', 12, listings_df, \"avail60\")\n",
    "listings_df = create_lagsleads('availability_90', 12, listings_df, \"avail90\")\n",
    "listings_df = create_lagsleads('availability_365', 12, listings_df, \"avail365\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_df = listings_df.reset_index(drop=True)\n",
    "\n",
    "# Count the number of unique months in the concatenated dataset\n",
    "Nunique_months = len(listings_df['month'].unique())\n",
    "\n",
    "# Create a clean measure for the number of reviews that prevents reviews from falling over time. This is done \n",
    "# so that changes in reviews can be used to get a sense of bookings.\n",
    "listings_df.loc[:, 'corrected_NOR'] = listings_df.groupby(['id'])['number_of_reviews'].rolling(Nunique_months, min_periods=1).max().reset_index(level=0, drop=True)\n",
    "listings_df = create_lagsleads('corrected_NOR', 12, listings_df, \"NOR\") \n",
    "listings_df.loc[:, \"NOR_diff\"] = listings_df['corrected_NOR'] - listings_df['NORlag1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment on the 'NOR_diff' variable\n",
    "* The measure for the differences in the number of reviews is imperfect but seems generally reasonable, researchers need to be careful with how it is used. \n",
    "\n",
    "* As long as properties appear in consecutive scrapes it seems to work well. If a property does not appear for many scrapes and reviews will sometimes jump significantly when it reappears. \n",
    "\n",
    "* When describing the 'NOR_diff' variable, we can observe the maximum number of reviews in a given month is 149! Which is not a reasonable number of reviews for a property to recieve in in 30/31 days. \n",
    "\n",
    "* It may be desirable to make an assumption that reviews are evenly across the months in which the property fails to appear in the scrape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    77545.000000\n",
      "mean         0.885615\n",
      "std          3.638718\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          0.000000\n",
      "75%          0.000000\n",
      "max        231.000000\n",
      "Name: NOR_diff, dtype: float64\n",
      "----\n",
      "99th percentile:\n",
      "17.0\n"
     ]
    }
   ],
   "source": [
    "# Describe the 'NOR_diff variable\n",
    "print(listings_df['NOR_diff'].describe())\n",
    "\n",
    "# Report 99th percentile of difference in number of reviews\n",
    "print(\"----\")\n",
    "print('99th percentile:')\n",
    "print(np.quantile(listings_df[~(listings_df['NOR_diff'].isna())]['NOR_diff'], 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [month, number_of_reviews, corrected_NOR, NOR_diff]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Example of what the corrected_NOR variable does. Can see that I don't allow the stock of reviews to decline.\n",
    "\n",
    "print(listings_df[listings_df['id'] == 10886050][['month', \n",
    "                                                  'number_of_reviews',\n",
    "                                                  'corrected_NOR', 'NOR_diff']].iloc[-19:-8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Exploring missing data and filling in NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checking_missing_data(var):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function checks what the difference between the number of unique ids and the \n",
    "    number of ids is paired with some property characteristic. \n",
    "    The key use case is to see whether or not a listing trait changes.\n",
    "    \"\"\"\n",
    "    \n",
    "    ids = listings_df[['id']].drop_duplicates().dropna()\n",
    "    ids = np.array(ids)\n",
    "\n",
    "    paired_ids = listings_df[['id', var]].dropna().drop_duplicates()\n",
    "    paired_ids = np.array(paired_ids)\n",
    "    \n",
    "    if len(ids) == len(paired_ids):\n",
    "        print('No change in variable')\n",
    "    else:\n",
    "        print(\"Variable changes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_variable_changes(var, cutoff, relate, df):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function lists ids where the variable of interest changes (\"var\").\n",
    "    This can be used for data cleaning purposes.\n",
    "    \"\"\"\n",
    "    \n",
    "    ops = {'>': operator.gt,\n",
    "       '<': operator.lt,\n",
    "       '>=': operator.ge,\n",
    "       '<=': operator.le,\n",
    "       '==': operator.eq}\n",
    "    \n",
    "    # Take ids and variable of interest and drop any na's\n",
    "    repetition_arr = np.array(df[['id', var]].dropna().drop_duplicates()) # Need drop_duplicates to identify actual price changes\n",
    "    counts = np.unique(repetition_arr[:,0], return_counts = True)\n",
    "    \n",
    "    return counts[0][ops[relate](counts[1], cutoff)], counts[1][ops[relate](counts[1], cutoff)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique ids that changed their official 'property type:'\n",
      "28839\n"
     ]
    }
   ],
   "source": [
    "# Example usage of the identify variable changes function\n",
    "\n",
    "# Store the ids for which the property type changes at least twice\n",
    "change_ids, change_counts = identify_variable_changes('property_type', 2, '>=', listings_df)\n",
    "\n",
    "# Unique ids that report changed property types\n",
    "print(\"Number of unique ids that changed their official 'property type:'\")\n",
    "print(len(change_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling in N/A's\n",
    "This section of the code fills missing data in forwards and backwards for values that would be expected to be invariant over time, such as fixed property features and host characteristics, during times for example when the host first started using Airbnb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the modal zip codes as a property's zip code\n",
    "# modal_zips = listings_df.groupby('id')['zipcode'].agg(lambda x: stats.mode(x)[0][0])\n",
    "# listings_df.loc[:, 'zipcode'] = modal_zips[listings_df['id']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumption: Properties do not change features when they are not observed\n",
    "Forward fill, then back fill. This means older features have priority."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9218/3371944126.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'host_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'host_name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'host_since'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'host_location'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'property_type'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'room_type'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bathrooms'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bedrooms'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'beds'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'first_review'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'instant_bookable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mlistings_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlistings_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ffill'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mlistings_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlistings_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bfill'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    946\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurried\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_apply_general\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurried\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_obj_with_exclusions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m_python_apply_general\u001b[0;34m(self, f, data)\u001b[0m\n\u001b[1;32m   1307\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mapplying\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \"\"\"\n\u001b[0;32m-> 1309\u001b[0;31m         \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmutated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrouper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m         return self._wrap_applied_output(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/groupby/ops.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, data, axis)\u001b[0m\n\u001b[1;32m    850\u001b[0m             \u001b[0;31m# group might be modified\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m             \u001b[0mgroup_axes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 852\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    853\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_indexed_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_axes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m                 \u001b[0mmutated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36mcurried\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mcurried\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 937\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m             \u001b[0;31m# preserve the name so we can detect it when calling plot methods,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mfillna\u001b[0;34m(self, value, method, axis, inplace, limit, downcast)\u001b[0m\n\u001b[1;32m   4820\u001b[0m             \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4821\u001b[0m             \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4822\u001b[0;31m             \u001b[0mdowncast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdowncast\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4823\u001b[0m         )\n\u001b[1;32m   4824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mfillna\u001b[0;34m(self, value, method, axis, inplace, limit, downcast)\u001b[0m\n\u001b[1;32m   6338\u001b[0m                 \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6339\u001b[0m                 \u001b[0mcoerce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6340\u001b[0;31m                 \u001b[0mdowncast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdowncast\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6341\u001b[0m             )\n\u001b[1;32m   6342\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36minterpolate\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"interpolate\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshift\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperiods\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, align_keys, ignore_failures, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m                     \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                     \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mignore_failures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36minterpolate\u001b[0;34m(self, method, axis, index, inplace, limit, limit_direction, limit_area, fill_value, coerce, downcast, **kwargs)\u001b[0m\n\u001b[1;32m   1112\u001b[0m             \u001b[0mlimit_area\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit_area\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m             \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         )\n\u001b[1;32m   1116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run a loop that fills property characteristics forwards and backwards\n",
    "\n",
    "my_timer = time.time() # Time it\n",
    "\n",
    "for var in ['host_id', 'host_name', 'host_since', 'host_location', 'property_type', 'room_type', 'bathrooms', 'bedrooms', 'beds', 'first_review', 'instant_bookable']:\n",
    "    listings_df.loc[:, var] = listings_df.groupby(['id'])[var].fillna(method='ffill', axis=0)\n",
    "    listings_df.loc[:, var] = listings_df.groupby(['id'])[var].fillna(method='bfill', axis=0)\n",
    "    \n",
    "time_to_run_filler =  time.time() - my_timer\n",
    "\n",
    "print(\"Minutes to run filling loop:\")\n",
    "print(time_to_run_filler/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dummies(var):\n",
    "    \"\"\"\n",
    "    Creates a dummy variable for a given variable\n",
    "    \"\"\"\n",
    "    listings_df.loc[:, var] = listings_df[var].astype('category')\n",
    "    listings_df.loc[:, var + \"_dum\"] = listings_df[var].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates dummy variables for specified categories\n",
    "\n",
    "categorical_vars = ['host_is_superhost', 'room_type', 'instant_bookable']\n",
    "\n",
    "for cat in categorical_vars:\n",
    "    create_dummies(cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Neighborhood categorization\n",
    "This section assigns each property to a specific neighborhood. There are two primary cases to be worried about:\n",
    "\n",
    "* 1) **Changing neighborhood:** A property with a changing neighborhood is simply assigned its modal neighborhood. \n",
    "* 2) **No neighborhood assigned:** A property without a reported neighborhood is assigned the neighborhood of the closest property that has a neighborhood reported."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding appropriate neighborhoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total listings:\")\n",
    "\n",
    "print(len(listings_df['neighbourhood_cleansed'].astype('category')))\n",
    "\n",
    "print(\"-------------------------\")\n",
    "print(\"Active listings with neighborhood reported:\")\n",
    "print(len(listings_df[(listings_df['List_month']==1) & (~listings_df['neighbourhood_cleansed'].isna())]))\n",
    "\n",
    "print(\"-------------------------\")\n",
    "print(\"Active listings with no neighborhood reported:\")\n",
    "print(len(listings_df[(listings_df['List_month']==1) & (listings_df['neighbourhood_cleansed'].isna())]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code replaces each listing's neighborhood with its modal neighborhood\n",
    "modal_neighs = listings_df.groupby('id')['neigh'].agg(lambda x: stats.mode(x)[0][0])\n",
    "listings_df.loc[:, 'neighbourhood_cleansed'] = modal_neighs[listings_df['id']].values\n",
    "\n",
    "# Creates a dummy for whether neighborhood was initially reported\n",
    "listings_df.loc[:, 'missing_neigh'] = (listings_df['neighbourhood_cleansed'] == 0)*1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average the reported longitude and latitude\n",
    "The longitude and latitude of a given property sometimes changes due to anonymization of Airbnb exact location. We just average these longitudes and latitudes to determine a representative location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_avg_lat, id_avg_lon = listings_df.groupby('id')['latitude'].mean(), listings_df.groupby('id')['longitude'].mean()\n",
    "\n",
    "listings_df.loc[:,'avg_lat'] = np.array(id_avg_lat[(listings_df['id'].values)])\n",
    "listings_df.loc[:, 'avg_lon'] = np.array(id_avg_lon[(listings_df['id'].values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set-up a dataframe of all of the listings missing a neighborhood\n",
    "missing_neigh = listings_df[(listings_df['List_month'] == 1) & (listings_df['neighbourhood_cleansed'] == 0) & (~listings_df['latitude'].isna()) & (~listings_df['longitude'].isna())][['id', 'avg_lat', 'avg_lon']]\n",
    "missing_neigh = missing_neigh.drop_duplicates()\n",
    "missing_neigh = missing_neigh.sort_index()\n",
    "missing_neigh = missing_neigh.reset_index(drop = False)\n",
    "\n",
    "# Dataframe of all the listings, that are not missing a neighborhood\n",
    "not_missing_neigh = listings_df[(listings_df['List_month'] == 1) & (listings_df['neighbourhood_cleansed'] != 0) & (~listings_df['latitude'].isna()) & (~listings_df['longitude'].isna())][['id', 'neighbourhood_cleansed','avg_lat', 'avg_lon']]\n",
    "not_missing_neigh = not_missing_neigh.drop_duplicates()\n",
    "\n",
    "# Print first five rows of the missing neighborhood dataframe\n",
    "missing_neigh.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_per_mi = 1.60934 #for distance conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(point1, point2):\n",
    "    \"\"\"\n",
    "    Calculate distance between two points\n",
    "    \"\"\"\n",
    "    return mpu.haversine_distance(point1, point2)\n",
    "\n",
    "def closest(data, this_point):\n",
    "    \"\"\"\n",
    "    Applies the distance function to each element in the data, \n",
    "    then returns the observation with the lowest distance.\n",
    "    \"\"\"\n",
    "    return min(data, key=lambda x: distance(this_point,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test distance function\n",
    "\n",
    "print(distance((30.170165, -97.756954), (30.277500,-97.713975))/km_per_mi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords_with_neigh = np.array(not_missing_neigh[['avg_lat', 'avg_lon']])\n",
    "coords_no_neigh = np.array(missing_neigh[['avg_lat', 'avg_lon']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neighborhood identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Identify neighborhoods for properties with no neighborhood assigned\n",
    "# # NOTE: IF THIS HAS BEEN RUN ONCE, CAN COMMENT OUT AND JUST LOAD IN 'approximated_neighs.csv'\n",
    "\n",
    "# neigh_timer = time.time()\n",
    "\n",
    "# approx_neighs = []\n",
    "\n",
    "# for i in coords_no_neigh:\n",
    "#      approx_neighs.append(not_missing_neigh[(not_missing_neigh['avg_lat'] == closest(tuple(coords_with_neigh), tuple(i))[0]) & (not_missing_neigh['avg_lon'] == closest(tuple(coords_with_neigh), tuple(i))[1])]['neighbourhood'].values[0])\n",
    "\n",
    "# missing_neigh['neighbourhood'] = approx_neighs\n",
    "\n",
    "# # Save the approximate neighborhoods so this code doesn't need to be run again.\n",
    "# missing_neigh.to_csv(city_abbrev + '_approximated_neighs.csv', index=False)\n",
    "\n",
    "# time_to_match = time.time() - neigh_timer\n",
    "\n",
    "# print(\"Mins to match neighborhoods:\")\n",
    "# print(time_to_match/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If approximated_neighs.csv exists, can load in neighborhoods here.\n",
    "# missing_neigh['neighbourhood'] = pd.read_csv(city_abbrev + '_approximated_neighs.csv')['neighbourhood']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with pd.option_context('mode.chained_assignment',None): # This just suppresses an innocous SettingWithCopy warning\n",
    "\n",
    "#     listings_df.loc[:, 'neighbourhood'][missing_neigh['index'].values] = missing_neigh['neighbourhood'].values.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all of the 0's with NaN's\n",
    "# listings_df.loc[:, 'neighbourhood'] = listings_df['neighbourhood'].replace({0: np.nan})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the neighborhood over the whole sample\n",
    "listings_df.loc[:,'neighbourhood_cleansed'] = listings_df.groupby(['id'])['neighbourhood_cleansed'].fillna(method='ffill', axis=0)\n",
    "listings_df.loc[:,'neighbourhood_cleansed'] = listings_df.groupby(['id'])['neighbourhood_cleansed'].fillna(method='bfill', axis=0)\n",
    "listings_df.loc[:,'neighbourhood_group_cleansed'] = listings_df.groupby(['id'])['neighbourhood_group_cleansed'].fillna(method='ffill', axis=0)\n",
    "listings_df.loc[:,'neighbourhood_group_cleansed'] = listings_df.groupby(['id'])['neighbourhood_group_cleansed'].fillna(method='bfill', axis=0)\n",
    "# Create neighborhood dummies - skipped\n",
    "# create_dummies('neighbourhood')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 'Calendar Update' Formatting\n",
    "\n",
    "Here we calculate a numeric value for the days since an Airbnb listing's calendar has been updated. This offers a measure for how active the Airbnb property is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cal_update = listings_df['calendar_updated'].str.split(\" \", n=2, expand=True)\n",
    "# cal_update.columns = ['count', 'units', 'numeric']\n",
    "\n",
    "# cal_update = cal_update[['count', 'units']] # Drop the third column\n",
    "# cal_update.loc[:, \"count\"] = cal_update['count'].replace({\"today\": 0, \"a\":1, \"yesterday\":0, \"never\":9999}).astype(float)\n",
    "# cal_update.loc[:, \"units\"] = cal_update['units'].replace({\"days\": 1, \"None\":1, \"weeks\":7, \"months\":30, \"week\":7}).astype(float)\n",
    "# cal_update.loc[:, 'days'] = cal_update['count']*cal_update['units']\n",
    "\n",
    "# cal_update.loc[cal_update['count']==0.0, \"days\"] = 0.0\n",
    "# cal_update.loc[cal_update['count']==9999, \"days\"] = 9999\n",
    "\n",
    "# # Add the calendar update values to the dataframe\n",
    "# listings_df.loc[:, 'days_since_calup'] = cal_update['days']\n",
    "# del cal_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the missing neighborhoods flag\n",
    "# listings_df = listings_df.drop(columns=['missing_neigh'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Additional variable creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flag month where a listing is first hosted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_host_ind = listings_df.groupby('id').List_month.idxmax()\n",
    "listings_df.loc[:, \"first_appearance\"] = (listings_df.index == first_host_ind[listings_df['id']]).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flag month where a listing is last hosted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_df_list = listings_df[listings_df['List_month'] == 1]\n",
    "last = listings_df_list.groupby('id')['month'].last()\n",
    "\n",
    "listings_df.loc[:, 'last_app'] = (listings_df['month'].values == last[listings_df['id']].values).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate cumulative listings for a given host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "host_cumlists = listings_df.groupby(['host_id', 'month'])['first_appearance'].sum().unstack().cumsum(axis=1).stack().astype(int)\n",
    "host_cumlists.name = 'cum_sum'\n",
    "listings_df = listings_df.join(host_cumlists, on=['host_id', 'month'], rsuffix='_cumsum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate other summary statistics about host holdings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Host listings per month\n",
    "listings_df = listings_df.join(listings_df.groupby(['host_id', 'month'])['List_month'].sum(), on=['host_id', 'month'], rsuffix='_byhost_month')\n",
    "\n",
    "# Host overall listings over the dataset\n",
    "listings_df = listings_df.join(listings_df.groupby(['host_id'])['List_month'].sum(), on=['host_id'], rsuffix='_host_overall')\n",
    "\n",
    "# Total times a given property is listed\n",
    "listings_df = listings_df.join(listings_df.groupby(['id'])['List_month'].sum(), on=['id'], rsuffix='_id_overall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify hotels in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_df.loc[:, 'hotel_dum'] = np.array((listings_df['property_type'] == \"Boutique hotel\") |\n",
    "                                  (listings_df['property_type'] == \"Bed and breakfast\") | \n",
    "                                  (listings_df['property_type'] == \"Boutique hotel\") | \n",
    "                                  (listings_df['property_type'] == \"Aparthotel\")| \n",
    "                                  (listings_df['property_type'] == \"Hotel\")| \n",
    "                                  (listings_df['property_type'] == \"Resort\")| \n",
    "                                  (listings_df['property_type'] == \"Serviced apartment\") )*1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure for an entrant to the Airbnb platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a preliminary measure for an entrant Airbnb listing.\n",
    "listings_df.loc[:,\"entrant\"] =  np.array((listings_df['first_appearance'] == 1) &\n",
    "                                (listings_df['days_since_first_rev'] < 30 ) & \n",
    "                                (listings_df['number_of_reviews'] < 10 ))*1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate listings per neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of listings in a neighborhood on the Airbnb platform for a given month\n",
    "listings_df = listings_df.join(listings_df.groupby(['neighbourhood_cleansed', 'month'])['List_month'].sum(), \n",
    "             on=['neighbourhood_cleansed', 'month'], rsuffix='_byneigh')\n",
    "\n",
    "# Calculate the lagged number of listings in a neighborhood on the Airbnb platform for a given month\n",
    "listings_df = listings_df.join(listings_df.sort_values(by=['neighbourhood_cleansed', 'month']).groupby(['neighbourhood_cleansed', 'month'])['List_month'].sum().shift(1), \n",
    "             on=['neighbourhood_cleansed', 'month'], rsuffix='_lag_byneigh')\n",
    "\n",
    "listings_df.loc[:,'List_month_lag_byneigh'] =  listings_df['List_month_lag_byneigh'].mask(listings_df['month'] == 3, np.nan)\n",
    "\n",
    "listings_df.sort_values(by=['neighbourhood_cleansed', 'month'])[['id', 'month', 'neighbourhood_cleansed', 'List_month_byneigh', 'List_month_lag_byneigh']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Creating drop criteria for observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 0-valued drop indicator. This will be replaced with 1 \n",
    "# whenever certain conditions are satisfied\n",
    "\n",
    "listings_df.loc[:, 'drop_indicator'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop criteria 1: Property *never* earns a review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the maximum difference in reviews by property\n",
    "max_NORdiff = listings_df.groupby('id')['NOR_diff'].max()\n",
    "listings_df.loc[:, 'max_NORdiff'] = max_NORdiff[listings_df['id']].values\n",
    "\n",
    "no_revs_ind = (listings_df['max_NORdiff'] == 0).values*1\n",
    "listings_df.loc[:, 'drop_indicator'] = listings_df['drop_indicator'].replace({ 0: no_revs_ind})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Drop criteria 2: Property price is below 0.1 percentile or above 99.9 percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_01per = listings_df.price.quantile(.001)\n",
    "price_999per = listings_df.price.quantile(.999)\n",
    "\n",
    "low_price = (listings_df.groupby('id')['price'].min()[listings_df['id']].values < price_01per)*1\n",
    "high_price = (listings_df.groupby('id')['price'].min()[listings_df['id']].values > price_999per)*1\n",
    "\n",
    "listings_df.loc[:, 'drop_indicator'] = listings_df['drop_indicator'].replace({ 0: low_price})\n",
    "listings_df.loc[:, 'drop_indicator'] = listings_df['drop_indicator'].replace({ 0: high_price})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Drop criteria 3: Property *never* lists a day of availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "never_avail = (listings_df.groupby('id')['availability_365'].max()[listings_df['id']].values == 0)*1\n",
    "listings_df.loc[:, 'drop_indicator'] = listings_df['drop_indicator'].replace({ 0: never_avail})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop criteria 4: Minimum nights is 30 days or more (no longer a short-term rental)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_term_rental = (listings_df.groupby(['id'])['minimum_nights'].min()[listings_df['id']].values >= 30)*1\n",
    "listings_df.loc[:, 'drop_indicator'] = listings_df['drop_indicator'].replace({ 0: long_term_rental})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Drop criteria 5: Hotel indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotel_ind = (listings_df.groupby(['id'])['hotel_dum'].max()[listings_df['id']].values == 1)*1\n",
    "listings_df.loc[:, 'drop_indicator'] = listings_df['drop_indicator'].replace({ 0: hotel_ind})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique properties flagged for drop:\")\n",
    "print(listings_df.groupby('id')['drop_indicator'].max().values.sum())\n",
    "\n",
    "print(\"Unique properties not flagged for drop:\")\n",
    "print((1 - listings_df.groupby('id')['drop_indicator'].max().values).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "# -- Save to compressed csv --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(csv_save_path)\n",
    "listings_dpf.to_csv(city_abbrev + '_1stStageClean_2021.csv.gz', compression='gzip', index=False, date_format='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu110.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1st Stage Panel Data Cleaning\n",
    "\n",
    "This notebook performs the first round of data cleaning tasks on the final long form data produced from the Aggregate Listings Data notebook. These tasks include:\n",
    "\n",
    "* 1. **Preliminary cleaning:** Includes de-stringing prices, formatting dates and computing variable lags and leads\n",
    "* 2. **Exploring missing values:** Creates functions for understanding NaN values and fills in time-invariant characteristics of a listing\n",
    "* 3. **Neighborhood categorization:** Assign neighborhoods to listings based on their GPS coordinates \n",
    "* 4. **'Calendar Update' formatting:** Creating numerical measure for how often an Airbnb listing is updated by the property host\n",
    "* 5. **Additional variable creation:** Additional variables are created for further anaylsis\n",
    "* 6. **Creating drop criteria for observations:** Primary focus of drop criteria is to drop dormant listings, price outliers and full-time hotels\n",
    "\n",
    "Once these tasks are complete, the Notebook saves the cleaned data into a compressed csv.gz file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import operator # This allows one to pass operators into a Python function\n",
    "import mpu # For distance calculation\n",
    "from scipy import stats # Used to find modal value of geographic values\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select city to work with\n",
    "\n",
    "city_folder = '/united-states_new-york-city'\n",
    "city_abbrev = 'NYC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Universal directory setup\n",
    "cwd1 = os.getcwd() \n",
    "\n",
    "# Go up one directory level\n",
    "os.chdir('..')\n",
    "cwd2 = os.getcwd()\n",
    "\n",
    "csv_raw_path = cwd2 + '/1. Download and compile data/'\n",
    "csv_save_path = cwd2 + '/Saved data/'\n",
    "\n",
    "# Revert to preliminary directory\n",
    "os.chdir(cwd1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     id  month  List_month last_scraped  host_id    host_name\n",
      "0  2595      0           1   2021-09-02   2845.0     Jennifer\n",
      "1  3831      0           1   2021-09-02   4869.0  LisaRoxanne\n",
      "2  5121      0           1   2021-09-02   7356.0        Garon\n",
      "3  5136      0           1   2021-09-02   7378.0      Rebecca\n",
      "4  5178      0           1   2021-09-02   8967.0     Shunichi\n",
      "5  5203      0           1   2021-09-02   7490.0    MaryEllen\n"
     ]
    }
   ],
   "source": [
    "# Read concatenated data\n",
    "os.chdir(csv_raw_path)\n",
    "\n",
    "listings_df = pd.read_csv(city_abbrev + '_Data_longALL_2021.csv.gz', low_memory=False)\n",
    "\n",
    "# Switch to other folder for saving data\n",
    "os.chdir(csv_save_path)\n",
    "\n",
    "# Show a snapshot of the dataframe\n",
    "print(listings_df.iloc[:6,:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'month', 'List_month', 'last_scraped', 'host_id', 'host_name',\n",
       "       'host_since', 'host_location', 'host_response_time',\n",
       "       'host_response_rate', 'host_is_superhost', 'host_listings_count',\n",
       "       'host_total_listings_count', 'neighbourhood', 'neighbourhood_cleansed',\n",
       "       'neighbourhood_group_cleansed', 'street', 'zipcode', 'latitude',\n",
       "       'longitude', 'is_location_exact', 'property_type', 'room_type',\n",
       "       'accommodates', 'bathrooms', 'bedrooms', 'beds', 'bed_type',\n",
       "       'square_feet', 'price', 'weekly_price', 'monthly_price',\n",
       "       'security_deposit', 'cleaning_fee', 'guests_included', 'extra_people',\n",
       "       'minimum_nights', 'maximum_nights', 'calendar_updated',\n",
       "       'calendar_last_scraped', 'has_availability', 'availability_30',\n",
       "       'availability_60', 'availability_90', 'availability_365',\n",
       "       'number_of_reviews', 'first_review', 'last_review',\n",
       "       'review_scores_rating', 'review_scores_accuracy',\n",
       "       'review_scores_cleanliness', 'review_scores_checkin',\n",
       "       'review_scores_communication', 'review_scores_location',\n",
       "       'review_scores_value', 'requires_license', 'license',\n",
       "       'instant_bookable', 'cancellation_policy',\n",
       "       'calculated_host_listings_count', 'reviews_per_month', 'amenities'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listings_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "# 1. Preliminary Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Destringing prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def destring_price(var):\n",
    "    \"\"\"\n",
    "    Destrings a passed variable.\n",
    "    \"\"\"\n",
    "    listings_df.loc[:, var] = listings_df[var].replace('[\\$,]', '', regex=True).astype(float)\n",
    "\n",
    "# This loop destrings price variables\n",
    "for var in ['price']:\n",
    "    \n",
    "    destring_price(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dates(var):\n",
    "    \"\"\"\n",
    "    This function converts date variables into datetime format.\n",
    "    \"\"\"\n",
    "    listings_df.loc[:, var] = pd.to_datetime(listings_df[var])\n",
    "    \n",
    "# ===============================================================    \n",
    "\n",
    "def dates_diff(var_name, var1, var2):\n",
    "    \"\"\"\n",
    "    Computes the difference between two date variables and assigns\n",
    "    the difference to a new variable of given name.\n",
    "    \"\"\"\n",
    "    listings_df.loc[:, var_name] = listings_df[var1] - listings_df[var2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a loop of date formatting\n",
    "for date_vars in ['last_scraped', 'host_since', 'first_review', 'last_review']:\n",
    "    format_dates(date_vars)\n",
    "    \n",
    "# Set the 'scrape_batch' as a modal date for a file being scraped in a CSV\n",
    "\n",
    "for m in listings_df['month'].unique():\n",
    "    listings_df.loc[listings_df['month'] == m, 'scrape_batch'] = listings_df[listings_df['month'] == m]['last_scraped'].mode().values[0]\n",
    "    listings_df.loc[:, 'scrape_batch'] = pd.to_datetime(listings_df['scrape_batch'])\n",
    "    \n",
    "# Create a Year-Month value for the scrape batch, this is largely used for graphing where one needs to aggregate by year-month\n",
    "listings_df.loc[:,\"batch_YRMO\"] = pd.to_datetime(listings_df['scrape_batch']).dt.to_period('M')    \n",
    "\n",
    "# Calculate different date differences\n",
    "dates_diff('days_since_rev', 'last_scraped', 'last_review')\n",
    "dates_diff('days_since_first_rev', 'last_scraped', 'first_review')\n",
    "dates_diff('host_length', 'last_scraped', 'host_since')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timedelta_formatter(var):\n",
    "    \"\"\"\n",
    "    This function formats the time delta for a passed variable.\n",
    "    \"\"\"\n",
    "    listings_df.loc[:, var] = pd.to_timedelta(listings_df[var]).dt.days\n",
    "    \n",
    "for deltas in ['days_since_rev', 'days_since_first_rev', 'host_length']:\n",
    "    timedelta_formatter(deltas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leads and lag creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment on the 'NOR_diff' variable\n",
    "* The measure for the differences in the number of reviews is imperfect but seems generally reasonable, researchers need to be careful with how it is used. \n",
    "\n",
    "* As long as properties appear in consecutive scrapes it seems to work well. If a property does not appear for many scrapes and reviews will sometimes jump significantly when it reappears. \n",
    "\n",
    "* When describing the 'NOR_diff' variable, we can observe the maximum number of reviews in a given month is 149! Which is not a reasonable number of reviews for a property to recieve in in 30/31 days. \n",
    "\n",
    "* It may be desirable to make an assumption that reviews are evenly across the months in which the property fails to appear in the scrape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Exploring missing data and filling in NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checking_missing_data(var):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function checks what the difference between the number of unique ids and the \n",
    "    number of ids is paired with some property characteristic. \n",
    "    The key use case is to see whether or not a listing trait changes.\n",
    "    \"\"\"\n",
    "    \n",
    "    ids = listings_df[['id']].drop_duplicates().dropna()\n",
    "    ids = np.array(ids)\n",
    "\n",
    "    paired_ids = listings_df[['id', var]].dropna().drop_duplicates()\n",
    "    paired_ids = np.array(paired_ids)\n",
    "    \n",
    "    if len(ids) == len(paired_ids):\n",
    "        print('No change in variable')\n",
    "    else:\n",
    "        print(\"Variable changes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_variable_changes(var, cutoff, relate, df):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function lists ids where the variable of interest changes (\"var\").\n",
    "    This can be used for data cleaning purposes.\n",
    "    \"\"\"\n",
    "    \n",
    "    ops = {'>': operator.gt,\n",
    "       '<': operator.lt,\n",
    "       '>=': operator.ge,\n",
    "       '<=': operator.le,\n",
    "       '==': operator.eq}\n",
    "    \n",
    "    # Take ids and variable of interest and drop any na's\n",
    "    repetition_arr = np.array(df[['id', var]].dropna().drop_duplicates()) # Need drop_duplicates to identify actual price changes\n",
    "    counts = np.unique(repetition_arr[:,0], return_counts = True)\n",
    "    \n",
    "    return counts[0][ops[relate](counts[1], cutoff)], counts[1][ops[relate](counts[1], cutoff)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique ids that changed their official 'property type:'\n",
      "28839\n"
     ]
    }
   ],
   "source": [
    "# Example usage of the identify variable changes function\n",
    "\n",
    "# Store the ids for which the property type changes at least twice\n",
    "change_ids, change_counts = identify_variable_changes('property_type', 2, '>=', listings_df)\n",
    "\n",
    "# Unique ids that report changed property types\n",
    "print(\"Number of unique ids that changed their official 'property type:'\")\n",
    "print(len(change_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling in N/A's\n",
    "This section of the code fills missing data in forwards and backwards for values that would be expected to be invariant over time, such as fixed property features and host characteristics, during times for example when the host first started using Airbnb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the modal zip codes as a property's zip code\n",
    "# modal_zips = listings_df.groupby('id')['zipcode'].agg(lambda x: stats.mode(x)[0][0])\n",
    "# listings_df.loc[:, 'zipcode'] = modal_zips[listings_df['id']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumption: Properties do not change features when they are not observed\n",
    "Forward fill, then back fill. This means older features have priority."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minutes to run filling loop:\n",
      "3.054268197218577\n"
     ]
    }
   ],
   "source": [
    "# Run a loop that fills property characteristics forwards and backwards\n",
    "\n",
    "my_timer = time.time() # Time it\n",
    "\n",
    "for var in ['host_id', 'host_name', 'host_since', 'host_location', 'property_type', 'room_type', 'bathrooms', 'bedrooms', 'beds', 'first_review', 'instant_bookable']:\n",
    "    listings_df.loc[:, var] = listings_df.groupby(['id'])[var].fillna(method='ffill', axis=0)\n",
    "    listings_df.loc[:, var] = listings_df.groupby(['id'])[var].fillna(method='bfill', axis=0)\n",
    "    \n",
    "time_to_run_filler =  time.time() - my_timer\n",
    "\n",
    "print(\"Minutes to run filling loop:\")\n",
    "print(time_to_run_filler/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dummies(var):\n",
    "    \"\"\"\n",
    "    Creates a dummy variable for a given variable\n",
    "    \"\"\"\n",
    "    listings_df.loc[:, var] = listings_df[var].astype('category')\n",
    "    listings_df.loc[:, var + \"_dum\"] = listings_df[var].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates dummy variables for specified categories\n",
    "\n",
    "categorical_vars = ['host_is_superhost', 'room_type', 'instant_bookable']\n",
    "\n",
    "for cat in categorical_vars:\n",
    "    create_dummies(cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Neighborhood categorization\n",
    "This section assigns each property to a specific neighborhood. There are two primary cases to be worried about:\n",
    "\n",
    "* 1) **Changing neighborhood:** A property with a changing neighborhood is simply assigned its modal neighborhood. \n",
    "* 2) **No neighborhood assigned:** A property without a reported neighborhood is assigned the neighborhood of the closest property that has a neighborhood reported."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding appropriate neighborhoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total listings:\n",
      "136278\n",
      "-------------------------\n",
      "Active listings with neighborhood reported:\n",
      "111430\n",
      "-------------------------\n",
      "Active listings with no neighborhood reported:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(\"Total listings:\")\n",
    "\n",
    "print(len(listings_df['neighbourhood_cleansed'].astype('category')))\n",
    "\n",
    "print(\"-------------------------\")\n",
    "print(\"Active listings with neighborhood reported:\")\n",
    "print(len(listings_df[(listings_df['List_month']==1) & (~listings_df['neighbourhood_cleansed'].isna())]))\n",
    "\n",
    "print(\"-------------------------\")\n",
    "print(\"Active listings with no neighborhood reported:\")\n",
    "print(len(listings_df[(listings_df['List_month']==1) & (listings_df['neighbourhood_cleansed'].isna())]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code replaces each listing's neighborhood with its modal neighborhood\n",
    "modal_neighs = listings_df.groupby('id')['neighbourhood_cleansed'].agg(lambda x: stats.mode(x)[0][0])\n",
    "listings_df.loc[:, 'neighbourhood_cleansed'] = modal_neighs[listings_df['id']].values\n",
    "\n",
    "# This code replaces each listing's neighborhood with its modal neighborhood\n",
    "modal_neighs = listings_df.groupby('id')['neighbourhood_cleansed'].agg(lambda x: stats.mode(x)[0][0])\n",
    "listings_df.loc[:, 'neighbourhood_group_cleansed'] = modal_neighs[listings_df['id']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average the reported longitude and latitude\n",
    "The longitude and latitude of a given property sometimes changes due to anonymization of Airbnb exact location. We just average these longitudes and latitudes to determine a representative location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_avg_lat, id_avg_lon = listings_df.groupby('id')['latitude'].mean(), listings_df.groupby('id')['longitude'].mean()\n",
    "\n",
    "listings_df.loc[:,'avg_lat'] = np.array(id_avg_lat[(listings_df['id'].values)])\n",
    "listings_df.loc[:, 'avg_lon'] = np.array(id_avg_lon[(listings_df['id'].values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>avg_lat</th>\n",
       "      <th>avg_lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [index, id, avg_lat, avg_lon]\n",
       "Index: []"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set-up a dataframe of all of the listings missing a neighborhood\n",
    "missing_neigh = listings_df[(listings_df['List_month'] == 1) & (listings_df['neighbourhood_cleansed'] == 0) & (~listings_df['latitude'].isna()) & (~listings_df['longitude'].isna())][['id', 'avg_lat', 'avg_lon']]\n",
    "missing_neigh = missing_neigh.drop_duplicates()\n",
    "missing_neigh = missing_neigh.sort_index()\n",
    "missing_neigh = missing_neigh.reset_index(drop = False)\n",
    "\n",
    "# Dataframe of all the listings, that are not missing a neighborhood\n",
    "not_missing_neigh = listings_df[(listings_df['List_month'] == 1) & (listings_df['neighbourhood_cleansed'] != 0) & (~listings_df['latitude'].isna()) & (~listings_df['longitude'].isna())][['id', 'neighbourhood_cleansed','avg_lat', 'avg_lon']]\n",
    "not_missing_neigh = not_missing_neigh.drop_duplicates()\n",
    "\n",
    "# Print first five rows of the missing neighborhood dataframe\n",
    "missing_neigh.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_per_mi = 1.60934 #for distance conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(point1, point2):\n",
    "    \"\"\"\n",
    "    Calculate distance between two points\n",
    "    \"\"\"\n",
    "    return mpu.haversine_distance(point1, point2)\n",
    "\n",
    "def closest(data, this_point):\n",
    "    \"\"\"\n",
    "    Applies the distance function to each element in the data, \n",
    "    then returns the observation with the lowest distance.\n",
    "    \"\"\"\n",
    "    return min(data, key=lambda x: distance(this_point,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.8474924451354475\n"
     ]
    }
   ],
   "source": [
    "# Test distance function\n",
    "\n",
    "print(distance((30.170165, -97.756954), (30.277500,-97.713975))/km_per_mi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords_with_neigh = np.array(not_missing_neigh[['avg_lat', 'avg_lon']])\n",
    "coords_no_neigh = np.array(missing_neigh[['avg_lat', 'avg_lon']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neighborhood identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mins to match neighborhoods:\n",
      "9.396870930989583e-05\n"
     ]
    }
   ],
   "source": [
    "# # Identify neighborhoods for properties with no neighborhood assigned\n",
    "# # NOTE: IF THIS HAS BEEN RUN ONCE, CAN COMMENT OUT AND JUST LOAD IN 'approximated_neighs.csv'\n",
    "\n",
    "neigh_timer = time.time()\n",
    "\n",
    "approx_neighs = []\n",
    "\n",
    "for i in coords_no_neigh:\n",
    "     approx_neighs.append(not_missing_neigh[(not_missing_neigh['avg_lat'] == closest(tuple(coords_with_neigh), tuple(i))[0]) & (not_missing_neigh['avg_lon'] == closest(tuple(coords_with_neigh), tuple(i))[1])]['neighbourhood_cleansed'].values[0])\n",
    "\n",
    "missing_neigh['neighbourhood_cleansed'] = approx_neighs\n",
    "\n",
    "# Save the approximate neighborhoods so this code doesn't need to be run again.\n",
    "missing_neigh.to_csv(city_abbrev + '_approximated_neighs.csv', index=False)\n",
    "\n",
    "time_to_match = time.time() - neigh_timer\n",
    "\n",
    "print(\"Mins to match neighborhoods:\")\n",
    "print(time_to_match/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If approximated_neighs.csv exists, can load in neighborhoods here.\n",
    "# missing_neigh['neighbourhood'] = pd.read_csv(city_abbrev + '_approximated_neighs.csv')['neighbourhood']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    " with pd.option_context('mode.chained_assignment',None): # This just suppresses an innocous SettingWithCopy warning\n",
    "\n",
    "     listings_df.loc[:, 'neighbourhood_cleansed'][missing_neigh['index'].values] = missing_neigh['neighbourhood_cleansed'].values.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all of the 0's with NaN's\n",
    "# listings_df.loc[:, 'neighbourhood'] = listings_df['neighbourhood'].replace({0: np.nan})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the neighborhood over the whole sample\n",
    "listings_df.loc[:,'neighbourhood_cleansed'] = listings_df.groupby(['id'])['neighbourhood_cleansed'].fillna(method='ffill', axis=0)\n",
    "listings_df.loc[:,'neighbourhood_cleansed'] = listings_df.groupby(['id'])['neighbourhood_cleansed'].fillna(method='bfill', axis=0)\n",
    "listings_df.loc[:,'neighbourhood_group_cleansed'] = listings_df.groupby(['id'])['neighbourhood_group_cleansed'].fillna(method='ffill', axis=0)\n",
    "listings_df.loc[:,'neighbourhood_group_cleansed'] = listings_df.groupby(['id'])['neighbourhood_group_cleansed'].fillna(method='bfill', axis=0)\n",
    "# Create neighborhood dummies - skipped\n",
    "# create_dummies('neighbourhood')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 'Calendar Update' Formatting\n",
    "\n",
    "Here we calculate a numeric value for the days since an Airbnb listing's calendar has been updated. This offers a measure for how active the Airbnb property is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cal_update = listings_df['calendar_updated'].str.split(\" \", n=2, expand=True)\n",
    "# cal_update.columns = ['count', 'units', 'numeric']\n",
    "\n",
    "# cal_update = cal_update[['count', 'units']] # Drop the third column\n",
    "# cal_update.loc[:, \"count\"] = cal_update['count'].replace({\"today\": 0, \"a\":1, \"yesterday\":0, \"never\":9999}).astype(float)\n",
    "# cal_update.loc[:, \"units\"] = cal_update['units'].replace({\"days\": 1, \"None\":1, \"weeks\":7, \"months\":30, \"week\":7}).astype(float)\n",
    "# cal_update.loc[:, 'days'] = cal_update['count']*cal_update['units']\n",
    "\n",
    "# cal_update.loc[cal_update['count']==0.0, \"days\"] = 0.0\n",
    "# cal_update.loc[cal_update['count']==9999, \"days\"] = 9999\n",
    "\n",
    "# # Add the calendar update values to the dataframe\n",
    "# listings_df.loc[:, 'days_since_calup'] = cal_update['days']\n",
    "# del cal_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the missing neighborhoods flag\n",
    "# listings_df = listings_df.drop(columns=['missing_neigh'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Additional variable creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flag month where a listing is first hosted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_host_ind = listings_df.groupby('id').List_month.idxmax()\n",
    "listings_df.loc[:, \"first_appearance\"] = (listings_df.index == first_host_ind[listings_df['id']]).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flag month where a listing is last hosted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_df_list = listings_df[listings_df['List_month'] == 1]\n",
    "last = listings_df_list.groupby('id')['month'].last()\n",
    "\n",
    "listings_df.loc[:, 'last_app'] = (listings_df['month'].values == last[listings_df['id']].values).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate cumulative listings for a given host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "host_cumlists = listings_df.groupby(['host_id', 'month'])['first_appearance'].sum().unstack().cumsum(axis=1).stack().astype(int)\n",
    "host_cumlists.name = 'cum_sum'\n",
    "listings_df = listings_df.join(host_cumlists, on=['host_id', 'month'], rsuffix='_cumsum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate other summary statistics about host holdings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Host listings per month\n",
    "listings_df = listings_df.join(listings_df.groupby(['host_id', 'month'])['List_month'].sum(), on=['host_id', 'month'], rsuffix='_byhost_month')\n",
    "\n",
    "# Host overall listings over the dataset\n",
    "listings_df = listings_df.join(listings_df.groupby(['host_id'])['List_month'].sum(), on=['host_id'], rsuffix='_host_overall')\n",
    "\n",
    "# Total times a given property is listed\n",
    "listings_df = listings_df.join(listings_df.groupby(['id'])['List_month'].sum(), on=['id'], rsuffix='_id_overall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify hotels in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_df.loc[:, 'hotel_dum'] = np.array((listings_df['property_type'] == \"Boutique hotel\") |\n",
    "                                  (listings_df['property_type'] == \"Bed and breakfast\") | \n",
    "                                  (listings_df['property_type'] == \"Boutique hotel\") | \n",
    "                                  (listings_df['property_type'] == \"Aparthotel\")| \n",
    "                                  (listings_df['property_type'] == \"Hotel\")| \n",
    "                                  (listings_df['property_type'] == \"Resort\")| \n",
    "                                  (listings_df['property_type'] == \"Serviced apartment\") )*1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure for an entrant to the Airbnb platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a preliminary measure for an entrant Airbnb listing.\n",
    "listings_df.loc[:,\"entrant\"] =  np.array((listings_df['first_appearance'] == 1) &\n",
    "                                (listings_df['days_since_first_rev'] < 30 ) & \n",
    "                                (listings_df['number_of_reviews'] < 10 ))*1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate listings per neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>month</th>\n",
       "      <th>neighbourhood_cleansed</th>\n",
       "      <th>List_month_byneigh</th>\n",
       "      <th>List_month_lag_byneigh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>182177</td>\n",
       "      <td>0</td>\n",
       "      <td>Allerton</td>\n",
       "      <td>32</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6286</th>\n",
       "      <td>8294259</td>\n",
       "      <td>0</td>\n",
       "      <td>Allerton</td>\n",
       "      <td>32</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9191</th>\n",
       "      <td>12496118</td>\n",
       "      <td>0</td>\n",
       "      <td>Allerton</td>\n",
       "      <td>32</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12375</th>\n",
       "      <td>17876530</td>\n",
       "      <td>0</td>\n",
       "      <td>Allerton</td>\n",
       "      <td>32</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12884</th>\n",
       "      <td>18852048</td>\n",
       "      <td>0</td>\n",
       "      <td>Allerton</td>\n",
       "      <td>32</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135235</th>\n",
       "      <td>53234241</td>\n",
       "      <td>2</td>\n",
       "      <td>Woodside</td>\n",
       "      <td>394</td>\n",
       "      <td>389.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135433</th>\n",
       "      <td>53321254</td>\n",
       "      <td>2</td>\n",
       "      <td>Woodside</td>\n",
       "      <td>394</td>\n",
       "      <td>389.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135437</th>\n",
       "      <td>53325837</td>\n",
       "      <td>2</td>\n",
       "      <td>Woodside</td>\n",
       "      <td>394</td>\n",
       "      <td>389.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135621</th>\n",
       "      <td>53413644</td>\n",
       "      <td>2</td>\n",
       "      <td>Woodside</td>\n",
       "      <td>394</td>\n",
       "      <td>389.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135891</th>\n",
       "      <td>53502370</td>\n",
       "      <td>2</td>\n",
       "      <td>Woodside</td>\n",
       "      <td>394</td>\n",
       "      <td>389.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>136278 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id  month neighbourhood_cleansed  List_month_byneigh  \\\n",
       "298       182177      0               Allerton                  32   \n",
       "6286     8294259      0               Allerton                  32   \n",
       "9191    12496118      0               Allerton                  32   \n",
       "12375   17876530      0               Allerton                  32   \n",
       "12884   18852048      0               Allerton                  32   \n",
       "...          ...    ...                    ...                 ...   \n",
       "135235  53234241      2               Woodside                 394   \n",
       "135433  53321254      2               Woodside                 394   \n",
       "135437  53325837      2               Woodside                 394   \n",
       "135621  53413644      2               Woodside                 394   \n",
       "135891  53502370      2               Woodside                 394   \n",
       "\n",
       "        List_month_lag_byneigh  \n",
       "298                        NaN  \n",
       "6286                       NaN  \n",
       "9191                       NaN  \n",
       "12375                      NaN  \n",
       "12884                      NaN  \n",
       "...                        ...  \n",
       "135235                   389.0  \n",
       "135433                   389.0  \n",
       "135437                   389.0  \n",
       "135621                   389.0  \n",
       "135891                   389.0  \n",
       "\n",
       "[136278 rows x 5 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the number of listings in a neighborhood on the Airbnb platform for a given month\n",
    "listings_df = listings_df.join(listings_df.groupby(['neighbourhood_cleansed', 'month'])['List_month'].sum(), \n",
    "             on=['neighbourhood_cleansed', 'month'], rsuffix='_byneigh')\n",
    "\n",
    "# Calculate the lagged number of listings in a neighborhood on the Airbnb platform for a given month\n",
    "listings_df = listings_df.join(listings_df.sort_values(by=['neighbourhood_cleansed', 'month']).groupby(['neighbourhood_cleansed', 'month'])['List_month'].sum().shift(1), \n",
    "             on=['neighbourhood_cleansed', 'month'], rsuffix='_lag_byneigh')\n",
    "\n",
    "listings_df.loc[:,'List_month_lag_byneigh'] =  listings_df['List_month_lag_byneigh'].mask(listings_df['month'] == 3, np.nan)\n",
    "\n",
    "listings_df.sort_values(by=['neighbourhood_cleansed', 'month'])[['id', 'month', 'neighbourhood_cleansed', 'List_month_byneigh', 'List_month_lag_byneigh']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Creating drop criteria for observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 0-valued drop indicator. This will be replaced with 1 \n",
    "# whenever certain conditions are satisfied\n",
    "\n",
    "listings_df.loc[:, 'drop_indicator'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Drop criteria 2: Property price is below 0.1 percentile or above 99.9 percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_01per = listings_df.price.quantile(.001)\n",
    "price_999per = listings_df.price.quantile(.999)\n",
    "\n",
    "low_price = (listings_df.groupby('id')['price'].min()[listings_df['id']].values < price_01per)*1\n",
    "high_price = (listings_df.groupby('id')['price'].min()[listings_df['id']].values > price_999per)*1\n",
    "\n",
    "listings_df.loc[:, 'drop_indicator'] = listings_df['drop_indicator'].replace({ 0: low_price})\n",
    "listings_df.loc[:, 'drop_indicator'] = listings_df['drop_indicator'].replace({ 0: high_price})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Drop criteria 3: Property *never* lists a day of availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "never_avail = (listings_df.groupby('id')['availability_365'].max()[listings_df['id']].values == 0)*1\n",
    "listings_df.loc[:, 'drop_indicator'] = listings_df['drop_indicator'].replace({ 0: never_avail})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop criteria 4: Minimum nights is 30 days or more (no longer a short-term rental)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_term_rental = (listings_df.groupby(['id'])['minimum_nights'].min()[listings_df['id']].values >= 30)*1\n",
    "listings_df.loc[:, 'drop_indicator'] = listings_df['drop_indicator'].replace({ 0: long_term_rental})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique properties flagged for drop:\n",
      "28619\n",
      "Unique properties not flagged for drop:\n",
      "16807\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique properties flagged for drop:\")\n",
    "print(listings_df.groupby('id')['drop_indicator'].max().values.sum())\n",
    "\n",
    "print(\"Unique properties not flagged for drop:\")\n",
    "print((1 - listings_df.groupby('id')['drop_indicator'].max().values).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "# -- Save to compressed csv --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(csv_save_path)\n",
    "listings_df.to_csv(city_abbrev + '_1stStageClean_2021.csv.gz', compression='gzip', index=False, date_format='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "# -- Save to SQL table --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "DATABASE_USERNAME = os.environ[\"DATABASE_USERNAME\"]\n",
    "DATABASE_PASSWORD = os.environ[\"DATABASE_PASSWORD\"]\n",
    "DATABASE_HOST = os.environ[\"DATABASE_HOST\"]\n",
    "DATABASE_PORT = os.environ[\"DATABASE_PORT\"]\n",
    "DATABASE_DATABASE = os.environ[\"DATABASE_DATABASE\"]\n",
    "\n",
    "SQLALCHEMY_DATABASE_URL = f\"postgresql://{DATABASE_USERNAME}:{DATABASE_PASSWORD}@{DATABASE_HOST}:{DATABASE_PORT}/{DATABASE_DATABASE}\"\n",
    "\n",
    "db = create_engine(SQLALCHEMY_DATABASE_URL)\n",
    "conn = db.connect()\n",
    "\n",
    "pd.read_csv(csv_save_path + city_abbrev + '_1stStageClean_2021.csv.gz', low_memory = False).to_sql('listings2021', con=conn, if_exists='replace', index=False)\n",
    "\n",
    "#close connection\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu110.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m91"
  },
  "interpreter": {
   "hash": "c7ce189d40a4c10384cffeaa2e77a221c2e23434bd25f025d599c475405f60a1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate Listings Data\n",
    "\n",
    "* This notebook concatenates Airbnb listing files from http://insideairbnb.com/ and creates both wide form and long form aggregate datasets.\n",
    "* The datasets are representative of a balanced panel for a given set of unique Airbnb listings. When a listing doesn't appear in a given month, it is still asssigned an entry in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "from itertools import compress\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Suppress innocuous warning about data fragmentation\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select city to work with\n",
    "\n",
    "city_folder = '/united-states_new-york-city'\n",
    "city_abbrev = 'NYC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store preliminary directory, use of os should make this compatible for any user with access to the repository\n",
    "cwd1 = os.getcwd() \n",
    "\n",
    "# Go up one directory level\n",
    "os.chdir('..')\n",
    "cwd2 = os.getcwd()\n",
    "\n",
    "# Make sure repository has a 0. Raw data folder!\n",
    "data_dir = cwd2 + '/0. Raw data' + city_folder\n",
    "\n",
    "# Revert to preliminary directory\n",
    "os.chdir(cwd1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function converts values into an integer, if it fails return a string.\n",
    "\n",
    "def IntorStr(value):\n",
    "    try:\n",
    "        return int(value)\n",
    "    except:\n",
    "        return str(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting listings.csv.gz files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new-york-city\n",
      "54\n",
      "['united-states_new-york-city_2015-01-01_listings.csv.gz'\n",
      " 'united-states_new-york-city_2015-03-01_listings.csv.gz'\n",
      " 'united-states_new-york-city_2015-05-01_listings.csv.gz'\n",
      " 'united-states_new-york-city_2015-06-01_listings.csv.gz'\n",
      " 'united-states_new-york-city_2015-08-01_listings.csv.gz']\n"
     ]
    }
   ],
   "source": [
    "# Collect the listings CSVs\n",
    "\n",
    "numFiles = []\n",
    "fileNames = os.listdir(data_dir)\n",
    "for fileNames in fileNames:\n",
    "    if fileNames.endswith(\"_listings.csv.gz\"):\n",
    "        numFiles.append(fileNames)\n",
    "    \n",
    "city = numFiles[0].split(\"_\")[1]\n",
    "print(city)\n",
    "\n",
    "# Count the number of files\n",
    "numFiles = np.sort(numFiles)\n",
    "print(len(numFiles))\n",
    "\n",
    "# Take a look at the first 5 listing files\n",
    "print(numFiles[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if a file is missing specific columns\n",
    "The loop below accepts a list of data file names and a list of column names and then prints if a file is missing a particular variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, take a look at columns in the first four files\n",
    "\n",
    "file_vars = [] # List of all variables in a scrape month\n",
    "N_file_vars = [] # Simple count of all variables in a scrape month\n",
    "\n",
    "for my_file in numFiles:\n",
    "    data = pd.read_csv(data_dir + '/' + my_file, encoding= 'iso-8859-1', low_memory = False)\n",
    "    data_columns = list(data.columns)\n",
    "    file_vars.append(data_columns)\n",
    "    N_file_vars.append(len(data_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date files to be dropped:\n",
      "['united-states_new-york-city_2015-01-01_listings.csv.gz'\n",
      " 'united-states_new-york-city_2015-03-01_listings.csv.gz'\n",
      " 'united-states_new-york-city_2015-05-01_listings.csv.gz'\n",
      " 'united-states_new-york-city_2015-06-01_listings.csv.gz']\n"
     ]
    }
   ],
   "source": [
    "# CREATE A DROP INDICATOR FOR DATA SCRAPES THAT ARE MISSING KEY VARIABLES\n",
    "\n",
    "counter = 0\n",
    "drop_list = []\n",
    "\n",
    "for date_vars in file_vars:\n",
    "    \n",
    "    # I need instant bookable to be included in my set of variables, could include many more conditions here\n",
    "    if 'instant_bookable' in date_vars:\n",
    "        pass \n",
    "    \n",
    "    else: \n",
    "        drop_list.append(counter)\n",
    "    \n",
    "    counter +=1\n",
    "    \n",
    "print(\"Date files to be dropped:\")\n",
    "print(numFiles[drop_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete date files that are missing variables of interest\n",
    "numFiles = np.delete(numFiles, drop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_cols(files, variables):\n",
    "    \n",
    "    \"\"\" \n",
    "    This function accepts a list of data file names (strings) \n",
    "    and a list of column names (strings) and then prints if \n",
    "    a file is missing a particular variable.\n",
    "    \"\"\"\n",
    "    \n",
    "    for my_file in files:\n",
    "        data = pd.read_csv(data_dir + '/' + my_file, encoding = 'iso-8859-1', low_memory = False)\n",
    "        data_columns = list(data.columns)\n",
    "        \n",
    "        for my_column in variables:\n",
    "            if my_column not in data_columns:\n",
    "                print(my_column + \" missing from:\")\n",
    "                print(my_file)\n",
    "        \n",
    "        print(\"------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loop is relatively slow. It takes about a minute and a half.\n",
    "\n",
    "def find_compatible_columns(most_missing_columns):\n",
    "    \n",
    "    \"\"\" \n",
    "    This function accepts a filename (string). The function then \n",
    "    returns a list of the column names that exist in all files\n",
    "    in numFiles based on the column names of the passed filename.\n",
    "    \"\"\"\n",
    "    \n",
    "    a = set(most_missing_columns.columns)\n",
    "    \n",
    "    for i in numFiles:\n",
    "        data = pd.read_csv(data_dir + '/'  + i, encoding = 'iso-8859-1', low_memory = False)\n",
    "        data_columns = set(data.columns)\n",
    "    \n",
    "        if i == numFiles[0]:\n",
    "            compat = data_columns.intersection(a)\n",
    "        \n",
    "        compat = compat.intersection(data_columns)\n",
    "    \n",
    "    compat = list(compat)\n",
    "    \n",
    "    return compat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'united-states_new-york-city_2015-08-01_listings.csv.gz'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most missing - oldest listing csv in this case\n",
    "\n",
    "numFiles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This cell is city-specific. One needs to be careful if not using Portland! - changed to New York City\n",
    "\n",
    "\n",
    "# This feeds the file with the most missing columns into the above function to create a compatible column set for all the data..\n",
    "most_missing = pd.read_csv(data_dir + '/united-states_new-york-city_2015-08-01_listings.csv.gz', encoding='iso-8859-1', low_memory = False)\n",
    "compatible = find_compatible_columns(most_missing)\n",
    "\n",
    "# Make id the first cell\n",
    "compatible.remove('id')\n",
    "compatible.insert(0, 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code creates and displays a list of important variables not found in the compatible set.\n",
    "\n",
    "airbnb_metrics = ['id', 'last_scraped', 'host_id', 'host_name', \n",
    "                  'host_since', 'host_location', 'host_response_time', 'host_response_rate',\n",
    "                  'host_is_superhost', 'host_listings_count', 'host_total_listings_count', 'neighbourhood',\n",
    "                  'neighbourhood_cleansed', 'street', 'zipcode', 'latitude', \n",
    "                  'longitude', 'is_location_exact', 'property_type', 'room_type', \n",
    "                  'accommodates', 'bathrooms', 'bedrooms', 'beds', \n",
    "                  'bed_type', 'square_feet', 'price', 'weekly_price',\n",
    "                  'monthly_price', 'security_deposit', 'cleaning_fee', 'guests_included',\n",
    "                  'extra_people', 'minimum_nights', 'maximum_nights', 'calendar_updated', \n",
    "                  'calendar_last_scraped', 'has_availability', 'availability_30', 'availability_60', \n",
    "                  'availability_90', 'availability_365', 'number_of_reviews', 'first_review', \n",
    "                  'last_review', 'review_scores_rating', 'review_scores_accuracy', 'review_scores_cleanliness', \n",
    "                  'review_scores_checkin', 'review_scores_communication', 'review_scores_location', 'review_scores_value', \n",
    "                  'requires_license', 'license', 'instant_bookable', 'cancellation_policy',\n",
    "                  'calculated_host_listings_count', 'reviews_per_month', 'amenities']\n",
    "\n",
    "airbnb_metrics_vs_compatible = list(filter(lambda i: i not in compatible, airbnb_metrics))\n",
    "airbnb_metrics_vs_compatible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Uncomment to print which datasets are missing the airbnb metrics data.\n",
    "# check_data_cols(numFiles, airbnb_metrics_vs_compatible)\n",
    "# none for our datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015\n",
      "united-states_new-york-city_2015-08-01_listings.csv.gz\n",
      "united-states_new-york-city_2015-09-01_listings.csv.gz\n",
      "united-states_new-york-city_2015-10-01_listings.csv.gz\n",
      "united-states_new-york-city_2015-11-01_listings.csv.gz\n",
      "united-states_new-york-city_2015-11-20_listings.csv.gz\n",
      "united-states_new-york-city_2015-12-02_listings.csv.gz\n",
      "--------------------------------------------\n",
      "2015\n",
      "united-states_new-york-city_2015-08-01_listings.csv.gz\n",
      "united-states_new-york-city_2015-09-01_listings.csv.gz\n",
      "united-states_new-york-city_2015-10-01_listings.csv.gz\n",
      "united-states_new-york-city_2015-11-01_listings.csv.gz\n",
      "united-states_new-york-city_2015-11-20_listings.csv.gz\n",
      "united-states_new-york-city_2015-12-02_listings.csv.gz\n",
      "--------------------------------------------\n",
      "2016\n",
      "united-states_new-york-city_2016-01-01_listings.csv.gz\n",
      "united-states_new-york-city_2016-02-02_listings.csv.gz\n",
      "united-states_new-york-city_2016-04-03_listings.csv.gz\n",
      "united-states_new-york-city_2016-05-02_listings.csv.gz\n",
      "united-states_new-york-city_2016-06-02_listings.csv.gz\n",
      "united-states_new-york-city_2016-07-02_listings.csv.gz\n",
      "united-states_new-york-city_2016-08-02_listings.csv.gz\n",
      "united-states_new-york-city_2016-09-02_listings.csv.gz\n",
      "united-states_new-york-city_2016-10-01_listings.csv.gz\n",
      "united-states_new-york-city_2016-11-02_listings.csv.gz\n",
      "united-states_new-york-city_2016-12-03_listings.csv.gz\n",
      "--------------------------------------------\n",
      "2017\n",
      "united-states_new-york-city_2017-01-01_listings.csv.gz\n",
      "united-states_new-york-city_2017-02-02_listings.csv.gz\n",
      "united-states_new-york-city_2017-03-02_listings.csv.gz\n",
      "united-states_new-york-city_2017-04-02_listings.csv.gz\n",
      "united-states_new-york-city_2017-05-02_listings.csv.gz\n",
      "united-states_new-york-city_2017-06-02_listings.csv.gz\n",
      "united-states_new-york-city_2017-07-02_listings.csv.gz\n",
      "united-states_new-york-city_2017-08-02_listings.csv.gz\n",
      "united-states_new-york-city_2017-09-02_listings.csv.gz\n",
      "united-states_new-york-city_2017-10-02_listings.csv.gz\n",
      "united-states_new-york-city_2017-11-01_listings.csv.gz\n",
      "united-states_new-york-city_2017-12-02_listings.csv.gz\n",
      "--------------------------------------------\n",
      "2018\n",
      "united-states_new-york-city_2018-01-10_listings.csv.gz\n",
      "united-states_new-york-city_2018-02-02_listings.csv.gz\n",
      "united-states_new-york-city_2018-03-04_listings.csv.gz\n",
      "united-states_new-york-city_2018-04-06_listings.csv.gz\n",
      "united-states_new-york-city_2018-05-09_listings.csv.gz\n",
      "united-states_new-york-city_2018-06-03_listings.csv.gz\n",
      "united-states_new-york-city_2018-07-05_listings.csv.gz\n",
      "united-states_new-york-city_2018-08-06_listings.csv.gz\n",
      "united-states_new-york-city_2018-09-08_listings.csv.gz\n",
      "united-states_new-york-city_2018-10-03_listings.csv.gz\n",
      "united-states_new-york-city_2018-11-03_listings.csv.gz\n",
      "united-states_new-york-city_2018-12-06_listings.csv.gz\n",
      "--------------------------------------------\n",
      "2019\n",
      "united-states_new-york-city_2019-01-09_listings.csv.gz\n",
      "united-states_new-york-city_2019-02-01_listings.csv.gz\n",
      "united-states_new-york-city_2019-03-06_listings.csv.gz\n",
      "united-states_new-york-city_2019-04-03_listings.csv.gz\n",
      "united-states_new-york-city_2019-05-03_listings.csv.gz\n",
      "united-states_new-york-city_2019-06-02_listings.csv.gz\n",
      "united-states_new-york-city_2019-07-08_listings.csv.gz\n",
      "united-states_new-york-city_2019-08-06_listings.csv.gz\n",
      "united-states_new-york-city_2019-09-12_listings.csv.gz\n",
      "--------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Display files by year\n",
    "\n",
    "years = np.linspace(2015,2019, 6)\n",
    "\n",
    "for year in years:\n",
    "    print(str(int(year)))\n",
    "    for IND in range(len(numFiles)):\n",
    "            if numFiles[IND].split('_')[2][0:4] == str(int(year)):\n",
    "                print(numFiles[IND])\n",
    "    print(\"--------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_spreadsheets(concat_year):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function accepts a starting and ending index of numFiles \n",
    "    as arguments and returns a cocatanated dataframe of the csv data \n",
    "    corresponding to the inputted indexes.\n",
    "    \"\"\"  \n",
    "    yearly_numfiles_bool = []\n",
    "    for filename in numFiles:\n",
    "        if (filename.split('_')[2][0:4] == str(int(concat_year))):\n",
    "            yearly_numfiles_bool.append(True)\n",
    "        else:\n",
    "            yearly_numfiles_bool.append(False)\n",
    "    \n",
    "    sheets_df = []\n",
    "    yearly_numfiles = list(compress(numFiles, yearly_numfiles_bool))\n",
    "    for filename in yearly_numfiles:\n",
    "        df = pd.read_csv(data_dir + '/' + filename, index_col = None, header=0, encoding='iso-8859-1', dtype={'id': \"Int64\"}, \n",
    "            na_values = ['I travel often :)', 'lady from NYC', \n",
    "            \"I love hosting on Airbnb and canât wait to meet you! \"], # some rows have weird values for the id, making them NaN\n",
    "            low_memory = False)\n",
    "        sheets_df.append(df)\n",
    "        \n",
    "    sheets_df = pd.concat(sheets_df, axis=0, ignore_index=True)\n",
    "    return sheets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code concatenates all datframes for a given year.\n",
    "\n",
    "# Save the concatenated frames as separate files\n",
    "sheet15 = concat_spreadsheets(2015) # 2015\n",
    "sheet16 = concat_spreadsheets(2016) # 2016\n",
    "sheet17 = concat_spreadsheets(2017) # 2017\n",
    "sheet18 = concat_spreadsheets(2018) # 2018\n",
    "sheet19 = concat_spreadsheets(2019) # 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IntegerArray>\n",
      "[ 4622922,   958444,  3943387,  3362669,  6627449,  5557381,  1886820,\n",
      "  4462008,   260566,   182177,\n",
      " ...\n",
      " 38557459, 38557492, 38558695, 38559180, 38563645, 38564068, 38564524,\n",
      " 38566777, 38567542, 38568081]\n",
      "Length: 171037, dtype: Int64\n"
     ]
    }
   ],
   "source": [
    "# Identify all unique listings ids across datasets, dropping the NAs\n",
    "uniq_all = pd.concat([sheet15[sheet15.id.notna()].id, sheet16[sheet16.id.notna()].id, \n",
    "                      sheet17[sheet17.id.notna()].id, sheet18[sheet18.id.notna()].id, \n",
    "                      sheet19[sheet19.id.notna()].id], axis=0, ignore_index=True).unique()\n",
    "\n",
    "print(uniq_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create wide form of full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wide_form(UNIQ_IDS, START, END, METRICS): \n",
    "    \n",
    "    \"\"\"\n",
    "    This function takes a list of Airbnb unique ids as well as start\n",
    "    and end indexes for said list. Then, for the selected ID's\n",
    "    it returns a dataframe of the relevant data in a wide format.\n",
    "    \n",
    "    Note on efficiency: Still getting warning that DataFrame is highly fragmented.\n",
    "                        Could still make this function a bit more efficient.\n",
    "    \"\"\"\n",
    "    \n",
    "    listing_df = pd.DataFrame(UNIQ_IDS)\n",
    "    listing_df.columns = ['id']\n",
    "    \n",
    "    print(\"Number of unique listings: \" + str(len(listing_df)))\n",
    "    \n",
    "    output_df = listing_df.copy()    \n",
    "    \n",
    "    date_count = START\n",
    "    for i in numFiles[START:END]:\n",
    " \n",
    "        # Read in gzip compressed files\n",
    "        file = gzip.open(os.path.join(data_dir, i), 'rt')  \n",
    "        reader = csv.reader(file, delimiter=',')\n",
    "        headers = next(reader)\n",
    "\n",
    "        bnb_metrics = METRICS\n",
    "        \n",
    "        d={}\n",
    "        for j in bnb_metrics:\n",
    "            d[str(j)+\"_index\"] = headers.index(j)\n",
    "            # print(str(j))\n",
    "            # print(d[str(j)+\"_index\"])\n",
    "\n",
    "        row_values = []\n",
    "\n",
    "        for row in reader:\n",
    "            value_i = []\n",
    "\n",
    "            for j in bnb_metrics:\n",
    "                try:\n",
    "                    value_j = IntorStr(row[d[str(j)+\"_index\"]])\n",
    "                except:\n",
    "                    value_j = np.NaN\n",
    "                value_i.append(value_j)\n",
    "\n",
    "            row_values.append(value_i)\n",
    "        \n",
    "        values_df = pd.DataFrame(row_values) # Create a dataframe for the row_values      \n",
    "        values_df.columns = bnb_metrics # Set column titles\n",
    "        values_df = values_df.drop_duplicates(subset='id', keep='last')\n",
    "\n",
    "        # Merge the values with their respective id and drop duplicates\n",
    "        merged_df = pd.merge(listing_df, values_df, how='outer', on='id')\n",
    "        merged_df = merged_df.drop_duplicates(keep='first')   \n",
    "        merged_df = merged_df.reset_index()\n",
    "\n",
    "        for k in bnb_metrics[1:]: \n",
    "            output_df[k + str(date_count)] = merged_df[k].copy() \n",
    "\n",
    "        output_df['List_month'+str(date_count)] = listing_df['id'].copy().isin(np.array(values_df['id']))*1 # See if the observation is in the month data  \n",
    "\n",
    "        date_count += 1\n",
    "        \n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique listings: 171037\n"
     ]
    }
   ],
   "source": [
    "# Wide format for all\n",
    "wideALL = wide_form(uniq_all, 0, len(numFiles), airbnb_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sl/twzsfyy90bq0k_r245410_100000gn/T/ipykernel_87398/1124768812.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Save wide dataframe to local directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m wideALL.to_csv(city_abbrev + '_Data_wideALL.csv.gz', \n\u001b[0m\u001b[1;32m      3\u001b[0m                compression = 'gzip', index=False)\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3464\u001b[0m         )\n\u001b[1;32m   3465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3466\u001b[0;31m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[1;32m   3467\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3468\u001b[0m             \u001b[0mline_terminator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mline_terminator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1103\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         )\n\u001b[0;32m-> 1105\u001b[0;31m         \u001b[0mcsv_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    255\u001b[0m             )\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_need_to_save_header\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save_body\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstart_i\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_i\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save_chunk\u001b[0;34m(self, start_i, end_i)\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mslicer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_native_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_number_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miget_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mto_native_types\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0mformatting\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrepr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \"\"\"\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"to_native_types\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mis_consolidated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, align_keys, ignore_failures, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m                     \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                     \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mignore_failures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mto_native_types\u001b[0;34m(self, na_rep, quoting, **kwargs)\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mto_native_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_rep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"nan\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquoting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0;34m\"\"\"convert to our native types format\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_native_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_rep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_rep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquoting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquoting\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mto_native_types\u001b[0;34m(values, na_rep, quoting, float_format, decimal, **kwargs)\u001b[0m\n\u001b[1;32m   2096\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2098\u001b[0;31m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2099\u001b[0m         \u001b[0mitemsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwriters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_len\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mna_rep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/dtypes/missing.py\u001b[0m in \u001b[0;36misna\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0mName\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \"\"\"\n\u001b[0;32m--> 138\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_isna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/dtypes/missing.py\u001b[0m in \u001b[0;36m_isna\u001b[0;34m(obj, inf_as_na)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCExtensionArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_isna_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minf_as_na\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minf_as_na\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mABCSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_isna_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minf_as_na\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minf_as_na\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/dtypes/missing.py\u001b[0m in \u001b[0;36m_isna_array\u001b[0;34m(values, inf_as_na)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_isna_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mArrayLike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minf_as_na\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m     \"\"\"\n\u001b[1;32m    223\u001b[0m     \u001b[0mReturn\u001b[0m \u001b[0man\u001b[0m \u001b[0marray\u001b[0m \u001b[0mindicating\u001b[0m \u001b[0mwhich\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minput\u001b[0m \u001b[0marray\u001b[0m \u001b[0mare\u001b[0m \u001b[0mNaN\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mNA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Save wide dataframe to local directory\n",
    "wideALL.to_csv(city_abbrev + '_Data_wideALL.csv.gz', \n",
    "               compression = 'gzip', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create long form of full data and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def long_form(df, METRICS):\n",
    "    \"\"\"\n",
    "    Creates a long form data frame when provided with \n",
    "    a wide form dataset.\n",
    "    \"\"\"\n",
    "    bnb_metrics = METRICS\n",
    "    mylist = ['List_month']\n",
    "    mylist.extend(bnb_metrics[1:])\n",
    "    long_df = pd.wide_to_long(df, stubnames=mylist, i='id', j='month')\n",
    "    \n",
    "   \n",
    "    return long_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full long dataframe creation\n",
    "longALL = long_form(wideALL, airbnb_metrics)\n",
    "\n",
    "# Resets the long dataframe's index\n",
    "longALL = longALL.reset_index()\n",
    "\n",
    "# Save long dataframe to local directory\n",
    "longALL.to_csv(city_abbrev + '_Data_longALL.csv.gz', \n",
    "               compression='gzip', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c7ce189d40a4c10384cffeaa2e77a221c2e23434bd25f025d599c475405f60a1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

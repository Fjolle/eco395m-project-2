{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate Listings Data\n",
    "\n",
    "* This notebook concatenates Airbnb listing files from http://insideairbnb.com/ and creates both wide form and long form aggregate datasets.\n",
    "* The datasets are representative of a balanced panel for a given set of unique Airbnb listings. When a listing doesn't appear in a given month, it is still asssigned an entry in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "from itertools import compress\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Suppress innocuous warning about data fragmentation\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select city to work with\n",
    "\n",
    "city_folder = '/united-states_new-york-city'\n",
    "city_abbrev = 'NYC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store preliminary directory, use of os should make this compatible for any user with access to the repository\n",
    "cwd1 = os.getcwd() \n",
    "\n",
    "# Go up one directory level\n",
    "os.chdir('..')\n",
    "cwd2 = os.getcwd()\n",
    "\n",
    "# Make sure repository has a 0. Raw data folder!\n",
    "data_dir = cwd2 + '/0. Raw data' + city_folder\n",
    "\n",
    "# Revert to preliminary directory\n",
    "os.chdir(cwd1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function converts values into an integer, if it fails return a string.\n",
    "\n",
    "def IntorStr(value):\n",
    "    try:\n",
    "        return int(value)\n",
    "    except:\n",
    "        return str(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting listings.csv.gz files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new-york-city\n",
      "3\n",
      "['united-states_new-york-city_2021-06-02_listings.csv.gz'\n",
      " 'united-states_new-york-city_2021-09-01_listings.csv.gz'\n",
      " 'united-states_new-york-city_2021-12-04_listings.csv.gz']\n"
     ]
    }
   ],
   "source": [
    "# Collect the listings CSVs\n",
    "\n",
    "numFiles = []\n",
    "fileNames = os.listdir(data_dir)\n",
    "for fileNames in fileNames:\n",
    "    if fileNames.endswith(\"_listings.csv.gz\"):\n",
    "        numFiles.append(fileNames)\n",
    "    \n",
    "city = numFiles[0].split(\"_\")[1]\n",
    "print(city)\n",
    "\n",
    "# Count the number of files\n",
    "numFiles = np.sort(numFiles)\n",
    "print(len(numFiles))\n",
    "\n",
    "# Take a look at the first 5 listing files\n",
    "print(numFiles[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if a file is missing specific columns\n",
    "The loop below accepts a list of data file names and a list of column names and then prints if a file is missing a particular variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, take a look at columns in the first four files\n",
    "\n",
    "file_vars = [] # List of all variables in a scrape month\n",
    "N_file_vars = [] # Simple count of all variables in a scrape month\n",
    "\n",
    "for my_file in numFiles:\n",
    "    data = pd.read_csv(data_dir + '/' + my_file, encoding= 'iso-8859-1', low_memory = False)\n",
    "    data_columns = list(data.columns)\n",
    "    file_vars.append(data_columns)\n",
    "    N_file_vars.append(len(data_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date files to be dropped:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# CREATE A DROP INDICATOR FOR DATA SCRAPES THAT ARE MISSING KEY VARIABLES\n",
    "\n",
    "counter = 0\n",
    "drop_list = []\n",
    "\n",
    "for date_vars in file_vars:\n",
    "    \n",
    "    # I need instant bookable to be included in my set of variables, could include many more conditions here\n",
    "    if 'instant_bookable' in date_vars:\n",
    "        pass \n",
    "    \n",
    "    else: \n",
    "        drop_list.append(counter)\n",
    "    \n",
    "    counter +=1\n",
    "    \n",
    "print(\"Date files to be dropped:\")\n",
    "print(numFiles[drop_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete date files that are missing variables of interest\n",
    "numFiles = np.delete(numFiles, drop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_cols(files, variables):\n",
    "    \n",
    "    \"\"\" \n",
    "    This function accepts a list of data file names (strings) \n",
    "    and a list of column names (strings) and then prints if \n",
    "    a file is missing a particular variable.\n",
    "    \"\"\"\n",
    "    \n",
    "    for my_file in files:\n",
    "        data = pd.read_csv(data_dir + '/' + my_file, encoding = 'iso-8859-1', low_memory = False)\n",
    "        data_columns = list(data.columns)\n",
    "        \n",
    "        for my_column in variables:\n",
    "            if my_column not in data_columns:\n",
    "                print(my_column + \" missing from:\")\n",
    "                print(my_file)\n",
    "        \n",
    "        print(\"------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loop is relatively slow. It takes about a minute and a half.\n",
    "\n",
    "def find_compatible_columns(most_missing_columns):\n",
    "    \n",
    "    \"\"\" \n",
    "    This function accepts a filename (string). The function then \n",
    "    returns a list of the column names that exist in all files\n",
    "    in numFiles based on the column names of the passed filename.\n",
    "    \"\"\"\n",
    "    \n",
    "    a = set(most_missing_columns.columns)\n",
    "    \n",
    "    for i in numFiles:\n",
    "        data = pd.read_csv(data_dir + '/'  + i, encoding = 'iso-8859-1', low_memory = False)\n",
    "        data_columns = set(data.columns)\n",
    "    \n",
    "        if i == numFiles[0]:\n",
    "            compat = data_columns.intersection(a)\n",
    "        \n",
    "        compat = compat.intersection(data_columns)\n",
    "    \n",
    "    compat = list(compat)\n",
    "    \n",
    "    return compat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This cell is city-specific. One needs to be careful if not using Portland! - changed to New York City\n",
    "\n",
    "\n",
    "# This feeds the file with the most missing columns into the above function to create a compatible column set for all the data..\n",
    "most_missing = pd.read_csv(data_dir + '/united-states_new-york-city_2021-06-02_listings.csv.gz', encoding='iso-8859-1', low_memory = False)\n",
    "compatible = find_compatible_columns(most_missing)\n",
    "\n",
    "# Make id the first cell\n",
    "compatible.remove('id')\n",
    "compatible.insert(0, 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'listing_url', 'scrape_id', 'last_scraped', 'name', 'description',\n",
       "       'neighborhood_overview', 'picture_url', 'host_id', 'host_url',\n",
       "       'host_name', 'host_since', 'host_location', 'host_about',\n",
       "       'host_response_time', 'host_response_rate', 'host_acceptance_rate',\n",
       "       'host_is_superhost', 'host_thumbnail_url', 'host_picture_url',\n",
       "       'host_neighbourhood', 'host_listings_count',\n",
       "       'host_total_listings_count', 'host_verifications',\n",
       "       'host_has_profile_pic', 'host_identity_verified', 'neighbourhood',\n",
       "       'neighbourhood_cleansed', 'neighbourhood_group_cleansed', 'latitude',\n",
       "       'longitude', 'property_type', 'room_type', 'accommodates', 'bathrooms',\n",
       "       'bathrooms_text', 'bedrooms', 'beds', 'amenities', 'price',\n",
       "       'minimum_nights', 'maximum_nights', 'minimum_minimum_nights',\n",
       "       'maximum_minimum_nights', 'minimum_maximum_nights',\n",
       "       'maximum_maximum_nights', 'minimum_nights_avg_ntm',\n",
       "       'maximum_nights_avg_ntm', 'calendar_updated', 'has_availability',\n",
       "       'availability_30', 'availability_60', 'availability_90',\n",
       "       'availability_365', 'calendar_last_scraped', 'number_of_reviews',\n",
       "       'number_of_reviews_ltm', 'number_of_reviews_l30d', 'first_review',\n",
       "       'last_review', 'review_scores_rating', 'review_scores_accuracy',\n",
       "       'review_scores_cleanliness', 'review_scores_checkin',\n",
       "       'review_scores_communication', 'review_scores_location',\n",
       "       'review_scores_value', 'license', 'instant_bookable',\n",
       "       'calculated_host_listings_count',\n",
       "       'calculated_host_listings_count_entire_homes',\n",
       "       'calculated_host_listings_count_private_rooms',\n",
       "       'calculated_host_listings_count_shared_rooms', 'reviews_per_month'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_missing.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compatible' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10433/4071174657.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m                   'calculated_host_listings_count', 'reviews_per_month', 'amenities']\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mairbnb_metrics_vs_compatible\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcompatible\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mairbnb_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mairbnb_metrics_vs_compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_10433/4071174657.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(i)\u001b[0m\n\u001b[1;32m     17\u001b[0m                   'calculated_host_listings_count', 'reviews_per_month', 'amenities']\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mairbnb_metrics_vs_compatible\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcompatible\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mairbnb_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mairbnb_metrics_vs_compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compatible' is not defined"
     ]
    }
   ],
   "source": [
    "# This code creates and displays a list of important variables not found in the compatible set.\n",
    "\n",
    "airbnb_metrics = ['id', 'last_scraped', 'host_id', 'host_name', \n",
    "                  'host_since', 'host_location', 'host_response_time', 'host_response_rate',\n",
    "                  'host_is_superhost', 'host_listings_count', 'host_total_listings_count', 'neighbourhood',\n",
    "                  'neighbourhood_cleansed', 'neighbourhood_group_cleansed', 'street', 'zipcode', 'latitude', \n",
    "                  'longitude', 'is_location_exact', 'property_type', 'room_type', \n",
    "                  'accommodates', 'bathrooms', 'bedrooms', 'beds', \n",
    "                  'bed_type', 'square_feet', 'price', 'weekly_price',\n",
    "                  'monthly_price', 'security_deposit', 'cleaning_fee', 'guests_included',\n",
    "                  'extra_people', 'minimum_nights', 'maximum_nights', 'calendar_updated', \n",
    "                  'calendar_last_scraped', 'has_availability', 'availability_30', 'availability_60', \n",
    "                  'availability_90', 'availability_365', 'number_of_reviews', 'first_review', \n",
    "                  'last_review', 'review_scores_rating', 'review_scores_accuracy', 'review_scores_cleanliness', \n",
    "                  'review_scores_checkin', 'review_scores_communication', 'review_scores_location', 'review_scores_value', \n",
    "                  'requires_license', 'license', 'instant_bookable', 'cancellation_policy',\n",
    "                  'calculated_host_listings_count', 'reviews_per_month', 'amenities']\n",
    "\n",
    "airbnb_metrics_vs_compatible = list(filter(lambda i: i not in compatible, airbnb_metrics))\n",
    "airbnb_metrics_vs_compatible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exlcude those columns not in the datasets\n",
    "for x in airbnb_metrics_vs_compatible:\n",
    "    airbnb_metrics.remove(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Uncomment to print which datasets are missing the airbnb metrics data.\n",
    "# check_data_cols(numFiles, airbnb_metrics_vs_compatible)\n",
    "# none for our datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_spreadsheets(concat_year):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function accepts a starting and ending index of numFiles \n",
    "    as arguments and returns a cocatanated dataframe of the csv data \n",
    "    corresponding to the inputted indexes.\n",
    "    \"\"\"  \n",
    "    yearly_numfiles_bool = []\n",
    "    for filename in numFiles:\n",
    "        if (filename.split('_')[2][0:4] == str(int(concat_year))):\n",
    "            yearly_numfiles_bool.append(True)\n",
    "        else:\n",
    "            yearly_numfiles_bool.append(False)\n",
    "    \n",
    "    sheets_df = []\n",
    "    yearly_numfiles = list(compress(numFiles, yearly_numfiles_bool))\n",
    "    for filename in yearly_numfiles:\n",
    "        df = pd.read_csv(data_dir + '/' + filename, index_col = None, header=0, encoding='iso-8859-1', dtype={'id': \"Int64\"}, \n",
    "             # some rows have weird values for the id, making them NaN\n",
    "            low_memory = False)\n",
    "        sheets_df.append(df)\n",
    "        \n",
    "    sheets_df = pd.concat(sheets_df, axis=0, ignore_index=True)\n",
    "    return sheets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IntegerArray>\n",
      "[    2595,     3831,     5121,     5136,     5178,     5203,     5803,\n",
      "     6848,     6872,     6990,\n",
      " ...\n",
      " 53656904, 53657155, 53658866, 53660784, 53660977, 53662330, 53662542,\n",
      " 53662772, 53663081, 53665099]\n",
      "Length: 45426, dtype: Int64\n"
     ]
    }
   ],
   "source": [
    "# Identify all unique listings ids across datasets, dropping the NAs\n",
    "sheet21 = concat_spreadsheets(2021)\n",
    "uniq_all = sheet21[sheet21.id.notna()].id.unique()\n",
    "\n",
    "print(uniq_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create wide form of full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wide_form(UNIQ_IDS, START, END, METRICS): \n",
    "    \n",
    "    \"\"\"\n",
    "    This function takes a list of Airbnb unique ids as well as start\n",
    "    and end indexes for said list. Then, for the selected ID's\n",
    "    it returns a dataframe of the relevant data in a wide format.\n",
    "    \n",
    "    Note on efficiency: Still getting warning that DataFrame is highly fragmented.\n",
    "                        Could still make this function a bit more efficient.\n",
    "    \"\"\"\n",
    "    \n",
    "    listing_df = pd.DataFrame(UNIQ_IDS)\n",
    "    listing_df.columns = ['id']\n",
    "    \n",
    "    print(\"Number of unique listings: \" + str(len(listing_df)))\n",
    "    \n",
    "    output_df = listing_df.copy()    \n",
    "    \n",
    "    date_count = START\n",
    "    for i in numFiles[START:END]:\n",
    " \n",
    "        # Read in gzip compressed files\n",
    "        file = gzip.open(os.path.join(data_dir, i), 'rt')  \n",
    "        reader = csv.reader(file, delimiter=',')\n",
    "        headers = next(reader)\n",
    "\n",
    "        bnb_metrics = METRICS\n",
    "        \n",
    "        d={}\n",
    "        for j in bnb_metrics:\n",
    "            d[str(j)+\"_index\"] = headers.index(j)\n",
    "            # print(str(j))\n",
    "            # print(d[str(j)+\"_index\"])\n",
    "\n",
    "        row_values = []\n",
    "\n",
    "        for row in reader:\n",
    "            value_i = []\n",
    "\n",
    "            for j in bnb_metrics:\n",
    "                try:\n",
    "                    value_j = IntorStr(row[d[str(j)+\"_index\"]])\n",
    "                except:\n",
    "                    value_j = np.NaN\n",
    "                value_i.append(value_j)\n",
    "\n",
    "            row_values.append(value_i)\n",
    "        \n",
    "        values_df = pd.DataFrame(row_values) # Create a dataframe for the row_values      \n",
    "        values_df.columns = bnb_metrics # Set column titles\n",
    "        values_df = values_df.drop_duplicates(subset='id', keep='last')\n",
    "\n",
    "        # Merge the values with their respective id and drop duplicates\n",
    "        merged_df = pd.merge(listing_df, values_df, how='outer', on='id')\n",
    "        merged_df = merged_df.drop_duplicates(keep='first')   \n",
    "        merged_df = merged_df.reset_index()\n",
    "\n",
    "        for k in bnb_metrics[1:]: \n",
    "            output_df[k + str(date_count)] = merged_df[k].copy() \n",
    "\n",
    "        output_df['List_month'+str(date_count)] = listing_df['id'].copy().isin(np.array(values_df['id']))*1 # See if the observation is in the month data  \n",
    "\n",
    "        date_count += 1\n",
    "        \n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique listings: 45426\n"
     ]
    }
   ],
   "source": [
    "# Wide format for all\n",
    "wideALL = wide_form(uniq_all, 0, len(numFiles), airbnb_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save wide dataframe to local directory\n",
    "wideALL.to_csv(city_abbrev + '_Data_wideALL_2021.csv.gz', \n",
    "               compression = 'gzip', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create long form of full data and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full long dataframe creation, Save long dataframe to local directory\n",
    "mylist = ['List_month']\n",
    "mylist.extend(airbnb_metrics[1:])\n",
    "\n",
    "pd.wide_to_long(\n",
    "    pd.read_csv(city_abbrev + '_Data_wideALL_2021.csv.gz',low_memory = False),\n",
    "    stubnames=mylist, \n",
    "    i='id', j='month').reset_index().to_csv(city_abbrev + '_Data_longALL_2021.csv.gz', \n",
    "    compression='gzip', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu110.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m91"
  },
  "interpreter": {
   "hash": "c7ce189d40a4c10384cffeaa2e77a221c2e23434bd25f025d599c475405f60a1"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
